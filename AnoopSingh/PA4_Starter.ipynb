{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MGTA 466: Analytics Assignment 4 - Word Count on Amazon EMR\n",
    "\n",
    "---\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "- Work with **BookReviews_1M** dataset to set up the word count exercise as you did in PA2. \n",
    "    - This is only to get you started on AWS and you won't need to report anything on this dataset except the run-time of your code.\n",
    "- Find the top 50 words containing the letter 'z' (lowercase) and their counts based on word count for the **BookReviews_4M** dataset\n",
    "- Calculate average and standard deviation of execution times over 3 runs for these three settings:\n",
    "    1. BookReviews_1M - 1 master + 1 worker node \n",
    "    2. BookReviews_4M - 1 master + 1 worker node\n",
    "    3. BookReviews_4M - 1 master + 3 worker nodes\n",
    "##### **NOTE** - The worker nodes are also called core nodes when initializing them on AWS\n",
    "\n",
    "#### Submission on Gradescope:\n",
    "You need to submit the following three files under \"PA4\". Instructions to generate the csv files are given in their respective sections\n",
    "* The current notebook with outputs using **BookReviews_4M.json** dataset - **PA4_Starter.ipynb**\n",
    "IMPORTANT: Make sure that all expected outputs are present and the output of each cell matches the expected output. **Do not display your outputs under new cells**\n",
    "* csv file containing the 50 most frequently occurring words containing the letter 'z' and their counts for the **BookReviews_4M** dataset + column header - **50_words.csv** (Instructions given below)\n",
    "* csv file containing execution times for the below dataset and EMR cluster configuration - **exec_times.csv** (Instructions given below)\n",
    "    1. BookReviews_1M - 1 master + 1 worker node \n",
    "    2. BookReviews_4M - 1 master + 1 worker node\n",
    "    3. BookReviews_4M - 1 master + 3 worker nodes\n",
    "* Screenshot of terminated clusters, cluster creation time, cluster elapsed time, and your username.\n",
    "  \n",
    "#### IMPORTANT submission guidelines enforced by autograder. Please read carefully:\n",
    "  * Make sure that all the cells in this notebook are executed before submission\n",
    "  * Some cells are marked **DO NOT DELETE**. These cells cannot be deleted and the output of these cells will be used for autograding\n",
    "  * You can add cells or delete(NOT recommended) other cells, but the **Expected Output** for each of the tasks MUST be the output of the cells marked as such\n",
    "  * DO NOT print anything other than the *exact* expected output. Do not include any sentences describing the output. This is strictly enforced by the autograder which checks for an *exact* match of the expected output. For example, if you are expected to print the PySpark version:\n",
    "      * '10.9.8' - <span style=\"color:#093\">CORRECT</span>\n",
    "      * 'The PySpark version is 10.9.8' - <span style=\"color:#FF0000\">INCORRECT</span>\n",
    "  * You can add cells for printing debugging information anywhere, but do not print anything else in **Expected Output** cells other than the expected output for the task\n",
    "  * For any task that expects `n` rows of a dataframe, **ALWAYS pass `truncate=False` as a parameter to `DataFrame.show` function**\n",
    "  * **NOTE - Any points deducted for not following submission guidelines will NOT be considered for regrade requests**\n",
    "\n",
    "---\n",
    "\n",
    "Remember: when in doubt, read the documentation first. It's always helpful to search for the class that you're trying to work with, e.g. pyspark.sql.DataFrame.\n",
    "\n",
    "PySpark API Documentation: https://spark.apache.org/docs/latest/api/python/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Upload the 1M dataset to S3\n",
    "\n",
    "To make the datasets available to the EMR cluster, we need to upload the data files to Amazon S3. Follow these steps to do so:\n",
    "\n",
    "1. In the Amazon console, open the **Services** menu on the top left and select **S3**\n",
    "2. Create a bucket if you don't have one yet. Use the default settings, but your bucket name must be unique. \n",
    "3. Create a folder in your bucket, e.g. `data`, using the default settings. (Don't upload the data file to the root of the bucket; we'll also use this bucket for later assignments, so it's good to keep everything organized.)\n",
    "4. Enter the folder and upload the **.txt** file. Do NOT upload the zip, as Spark won't know what to do with it. \n",
    "\n",
    "---\n",
    "\n",
    "You can use this dataset now.\n",
    "\n",
    "\n",
    "This exercise is only to help you understand how you can create your own S3 buckets and read data from it. The actual task for you is to read data (BookReviews_4M.json) from a different S3 bucket and work on that dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up the EMR cluster and creating a PySpark notebook\n",
    "\n",
    "We have already uploaded the 4M reviews data to the s3 bucket `s3://mgta466-w25` under the `data` folder. Follow the steps in the Session 4 Demo to create an EMR cluster followed by a workspace for you to work on this notebook. \n",
    "After launching the workspace (that is attached to your EMR cluster) in JupyterLab:\n",
    "\n",
    "1. Upload this notebook(**PA4_Starter.ipynb**) in the workspace. **IMPORTANT** Click on *kernel* on the top right and select **PySpark**\n",
    "2. The `BookReviews_4M.json` data is at `s3://mgta466-w25/data/BookReviews_4M.json`. In the following sections, use this URI for data file path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start Spark Session and Read Data - 2 points\n",
    "\n",
    "Note that you don't need to manually start the spark session. AWS does it for you in the background, so that the spark session is started as soon as you import pyspark. The spark session is automatically available in the global variable `spark`\n",
    "\n",
    "Remember that the kernel for running this Notebook is **PySpark** and not Python 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected Output** - Spark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "31ebd2b5-f0f7-4f99-82a9-2ffffc401617",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Record the starting time of execution for timing this notebook\n",
    "\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#REPLACE WITH YOUR S3 PATH FOR BookReviews_1M.txt WHEN REQUIRED\n",
    "dataFileName = \"s3://mgta466-w25/data/BookReviews_4M.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data from S3. The data is present in the S3 path provided above\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected Output** - Number of rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f12e051b-7514-493e-b547-f1f8b5b64f86",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examine the data - 1 point\n",
    "\n",
    "Your task: Examine the contents of the dataframe that you've just read from file.\n",
    "##### Note: For step 2 below keep only the reviewText column in the DF and rename reviewText to value\n",
    "\n",
    "Expected output: \n",
    "\n",
    "1. Schema of the raw dataframe (with all columns)\n",
    "2. First 25 rows of the dataframe showing only whole sentence of reviewText with column renamed to 'value'. Use the `truncate` parameter of `DataFrame.show` to display whole sentences without truncation\n",
    "\n",
    "**Note**: If you're attempting to run the notebook with \"BookReviews_1M.txt\", please note that it lacks the \"reviewText\" column. Consequently, you'll need to make appropriate changes to Step 2 and then display the first 25 rows of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output** - Schema of the raw dataframe (with all columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e7f65258-9c39-432c-b54f-5fcdfd112bba",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output** - First 25 rows of the dataframe showing whole sentence under a column named `value`\n",
    "**REMINDER**: Always pass truncate=FALSE to dataframe show() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "34efb4be-584a-40e9-ae64-afa974dc29ea",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clean the data - 2 point\n",
    "\n",
    "Your task: Remove all punctuations and convert all characters to lower case.\n",
    "\n",
    "Expected output: The first 25 rows of a dataframe, with a column containing the cleaned sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not change this cell. \n",
    "\n",
    "# NOTE: Counterintuitively, column objects do NOT store any data; instead they store column expressions (transformations). \n",
    "#       The below function takes in a column object, and adds more expressions to it to make a more complex transformation. \n",
    "#       Once we have a column object representing the expressions we want, use DataFrame.select(column) to apply the expressions\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "def removePunctuation(column):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\"\"\"\n",
    "    return trim(lower(regexp_replace(column, \"[^A-Za-z0-9 ]\", \"\"))).alias(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recommended: take a look at the contents of a column object returned from removePunctuations. What's in there? \n",
    "# No answers or outputs required for this cell. \n",
    "print(removePunctuation(textDF.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output** - The first 25 rows of the cleaned dataframe, with a column containing the **entire** cleaned sentences, under a column named `sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a703fd94-9b20-43f4-925e-a1f879258ef7",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL\n",
    "# execute the column expressions generated by removePunctuation() to clean the sentences\n",
    "# After that, use the show() function to print the first 25 rows of the dataframe\n",
    "# Hint: you'll need the Column object returned by removePunctuations(). \n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Get dataframe containing unique words that contain the letter 'z' (lowercase) and their counts - 3 points\n",
    "\n",
    "#### 6.1 Create a dataframe of words - 1 point\n",
    "\n",
    "#### Tasks:\n",
    "1. Split each sentence into words based on the delimiter space (' ').\n",
    "2. Put each word in each sentence row into their own rows. Put your results into a new dataframe.\n",
    "3. Print out the first 5 rows of the dataframe.\n",
    "\n",
    "#### Expected output: \n",
    "1. Show first 5 rows of the output dataframe which would be:\n",
    "\n",
    "| word   |\n",
    "---------\n",
    "|my      |\n",
    "|daughter|\n",
    "|got     |\n",
    "|her     |\n",
    "|first   |\n",
    "    \n",
    "only showing top 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output** - First 5 rows of the word dataframe, under a column named `word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e9b3565f-b12e-44a7-b2e2-323f0165c5eb",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL\n",
    "# Assemble the 'split' and 'explode' column expressions, then apply them to the sentence column\n",
    "\n",
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Filter words that contain the letter 'z' and count them - 2 points\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "1. Remove all empty rows in the dataframe. These might have crept in because of the empty lines or words in the file.\n",
    "2. Filter the dataframe to contain only words that have the letter 'z' in them.\n",
    "2. Group rows in the previous dataframe by unique words, then count the rows in each group. Put your results into a new dataframe.\n",
    "\n",
    "#### Expected output:\n",
    "1. Show the first 25 rows of the dataframe, where each row contains only one word. The dataframe must not contain empty rows. \n",
    "2. Show 25 rows of the dataframe containing unique words and their counts\n",
    "\n",
    "##### The output after removing empty rows and filtering the dataframe would be similar to:\n",
    "\n",
    "\n",
    "|          word|\n",
    "----------------\n",
    "|       jazz|\n",
    "|         jazz|\n",
    "|      jazz|\n",
    "... 22 more\n",
    "\n",
    "##### The output after grouping unique words and counting the rows in each group would be similar to:\n",
    "\n",
    "\n",
    "|         word|count|\n",
    "--------------|------\n",
    "|arizona|1 |\n",
    "|huzzah         |18 |\n",
    "|zoo      |15   |\n",
    "... 22 more\n",
    "\n",
    "\n",
    "**NOTE** - The above tables are for illustration only. Your output may differ and should contain all 25 rows for each of the tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output** - First 25 rows of the dataframe containing words with the letter 'z' in them, under a column named `word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "057ad753-5fd8-480a-bea4-5bf061fc0760",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL\n",
    "# YOUR CODE HERE\n",
    "from pyspark.sql.functions import length, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output** - First 25 rows of the dataframe containing words with the letter 'z' in them and their counts, under `word` and `count` columns respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "cda2a850-6da0-468e-95d0-bb791fee8a5c",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Sort the word count dataframe in a descending order by count - 2 point\n",
    "\n",
    "Your task: Sort the previous dataframe by the counts column in a descending order. Put your results into a new dataframe. \n",
    "\n",
    "Expected output: First 25 rows of the sorted word count dataframe. The first row would have the maximum count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output** - First 25 rows of the sorted word count dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b0f29a71-d9a8-4d99-99ff-ddbe3cad509f",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Record the execution time\n",
    "\n",
    "Your task: Print the execution time.\n",
    "\n",
    "#### **Expected output** - The execution time. No particular value is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the time since execution start - You will need this value later.\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Save the sorted word counts directly to S3 as a CSV file - 1 point\n",
    "\n",
    "NOTE: Spark uses a distributed memory system, and stores working data in fragments known as \"partitions\". This is advantageous when a Spark cluster spans multiple machines, as each machine will only require part of the working data to do its own job. By default, Spark will save each of these data partitions into a individual file to avoid I/O collisions. We want only one output file, so we'll need to fuse all the data into a single partition first. \n",
    "\n",
    "Your task: \n",
    "1. Coalesce the previous dataframe to one partition. This makes sure that all our results will end up in the same CSV file. \n",
    "2. Save the 1-partition dataframe to S3 using the DataFrame.write.csv() method. Take note to store the file inside S3, at a place that you can remember. The save path should look something like `s3://<your-bucket>/<your-folder>/<your-result-file>.csv`. Change these parameters to point to your bucket and folder.\n",
    "3. Remember to save the csv file along with the header\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "#### You only need to run the section 9 and section 10 once for the 4M dataset.\n",
    "#### Section 11 requires you to run multiple iterations of this Notebook, and for that you can comment out the code in section 9 so that it's easier for you to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save results to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Submission of `50_rows.csv` - Download the CSV file from S3 to your local machine and create the expected CSV output file - 2 point\n",
    "\n",
    "1. Navigate to the S3 folder where you stored your output\n",
    "2. Note the name of this file, it should look something like `part-00000-xx.....xx.csv`. \n",
    "3. Click on this file, it should open the file properties.\n",
    "4. Click on 'Download'.\n",
    "5. After downloading the file, you can rename it to anthing, say `results.csv`. \n",
    "6. We want you to submit a CSV containing the first 50 rows of the results file. Remember that we want the first 51 lines which would include the header as well - so basically it is header + 50 rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Submission of `exec_times.csv` - Execution times on different dataset and settings - 2 point\n",
    "\n",
    "You need to experiment with using different number of master and worker nodes for running this whole Jupyter Notebook. You will have to report the execution time of this Notebook as you noted in an earlier section.\n",
    "\n",
    "1. Create a cluster with the required number of master and worker(core) nodes.\n",
    "2. Then go to the Kernel tab in JupyterLab, and do 'Restart and run all cells.'\n",
    "3. You should note the time in the cell just before section 9 - this is the time that it took for all the code to run.\n",
    "4. Then, start a new cluster with a different configuration of master and worker nodes and dataset as expected. Run the Notebook again, and note the execution times.\n",
    "5. You must change the dataFileName to read the BookReviews_1M.txt from your S3 bucket. Also note that BookReviews_1M.txt and BookReviews_4M.json have different data formats and must be handled appropriately.\n",
    "6. Create a csv file `exec_times.csv` and fill it in the following format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset | #Master Nodes | #Core Nodes | Runtime_1 | Runtime_2 | Runtime_3 | Mean | Std |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| 1M | 1 | 1 | | | | | | \n",
    "| 4M | 1 | 1 | | | | | |\n",
    "| 4M | 1 | 3 | | | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Screenshots of terminated EMR clusters -`cluster_ss.png`\n",
    "\n",
    "You need to attach a screenshot of your Amazon EMR 'Clusters' page which shows your terminated clusters, cluster creation time, cluster elapsed time, and your username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
